
## ðŸ”§ Technologies I Work With

* Backend: Python, Flask, FastAPI
* AI/ML: LangChain, LLMs (OpenAI, GPT, Claude), RAG, Vector Databases
* Cloud/Storage: AWS S3, Serverless
* DevOps: Docker, CI/CD

## ðŸš€ What Iâ€™m Building
* AI-powered document search and question-answering systems

* Scalable APIs for NLP applications

* Retrieval-Augmented Generation (RAG) pipelines

## ðŸŒ± Learning & Exploring
* Optimizing LLM performance & cost

* Open-source LLMs (Llama 2, Mistral)

* MLOps for generative AI 

(Customize links and add emojis for personality!)

## ðŸ’¡ Why This Works
* Concise: Highlights key skills without clutter.

* Action-Oriented: Focuses on projects and learning goals.

* Search-Friendly: Keywords like "RAG," "LangChain," and "LLMs" attract recruiters.

# How to run?

### STEPS:


### STEP 01- Create a conda environment after opening the repository

```bash
conda create -n llmapp python=3.11 -y
```

```bash
conda activate llmapp
```


### STEP 02- install the requirements
```bash
pip install -r requirements.txt
```


```bash
# Finally run the following command
python app/main.py
```

Now,
```bash
open up you local host and port
```



1) custom docs
2) extract data
3) chunking operation
4) embedding model
4) vector embedding
5) vector database (knowledge base)====== {cromadb, pinecone}  but used cromadb
6) retuns rankoutput/ response if k = 3 it will return 3 responses
7) giving context and query to the LLM and finally giving the response to the user


#  python
# flask (front-end)
# LLM 
# vector db
# S3 bucket (AWS)
# CICD pipeline for deployment



